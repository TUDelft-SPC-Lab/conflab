{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.notebook import trange\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from system import System\n",
    "from conflab.data_loading.pose import ConflabPoseExtractor\n",
    "from conflab.data_loading.accel import ConflabAccelExtractor\n",
    "from conflab.data_loading.person import ConflabDataset, ConflabSubset\n",
    "from conflab.data_loading.labels import ConflabLabelExtractor\n",
    "from conflab.constants import conflab_pose_path, midge_data_path, conflab_speaking_status_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fold(train_ds, test_ds, model_name='resnet', model_hparams={}, deterministic=False, log_prefix=None):\n",
    "    # split the train set into train and val\n",
    "    # use 10% of data for val\n",
    "    val_ds, train_ds = train_ds.random_split(0.1)\n",
    "\n",
    "    # data loaders\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=64, shuffle=True, num_workers=4,\n",
    "        collate_fn=None)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        val_ds, batch_size=128, shuffle=False, num_workers=4,\n",
    "        collate_fn=None)\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        test_ds, batch_size=128, shuffle=False, num_workers=4,\n",
    "        collate_fn=None)\n",
    "\n",
    "    system = System(model_name, model_hparams=model_hparams)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(dirpath=\"./checkpoints\", save_top_k=1, mode=\"max\", monitor=\"val_auc\")\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_auc\", patience=6, mode=\"max\"),\n",
    "            checkpoint_callback\n",
    "        ],\n",
    "        accelerator='gpu',\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=20,\n",
    "        deterministic=deterministic,\n",
    "        logger=pl.loggers.TensorBoardLogger(save_dir=\"logs/\", version=log_prefix))\n",
    "    trainer.fit(system, data_loader_train, data_loader_val)\n",
    "\n",
    "    trainer.test(system, data_loader_test, ckpt_path='best')\n",
    "    return system.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(outputs, labels, type='binary'):\n",
    "    if type == 'binary':\n",
    "        proba = torch.sigmoid(outputs)\n",
    "        pred = (proba > 0.5)\n",
    "\n",
    "        correct = pred.eq(labels).sum().item()\n",
    "        return {\n",
    "            'auc': roc_auc_score(labels, proba),\n",
    "            'acc': correct / len(outputs),\n",
    "            'correct': correct\n",
    "        }\n",
    "    elif type == 'regression':\n",
    "        return {\n",
    "            'mse': torch.nn.functional.mse_loss(outputs, labels, reduction='mean'),\n",
    "            'l1': torch.nn.functional.l1_loss(outputs, labels, reduction='mean')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_run(dataset, model_name, random_state, metrics_name='binary', deterministic=False, log_prefix='cv'):\n",
    "    # split per pid\n",
    "    pids = set(dataset.get_groups())\n",
    "    pid_splits = KFold(n_splits=10, random_state=random_state, shuffle=True).split(range(len(pids)))\n",
    "\n",
    "    outputs = torch.empty((len(dataset),))\n",
    "    for f, (train_pids, test_pids) in enumerate(pid_splits):\n",
    "        # create datasets   \n",
    "        train_idx = [i for i, e in enumerate(dataset.examples) if e[0] in train_pids]\n",
    "        test_idx = [i for i, e in enumerate(dataset.examples) if e[0] in test_pids]\n",
    "        print(f'ds split into {len(train_idx)} train and {len(test_idx)} test')\n",
    "\n",
    "        train_ds = ConflabSubset(dataset, train_idx)\n",
    "        test_ds = ConflabSubset(dataset, test_idx)\n",
    "\n",
    "        c_in = dataset.extractors['accel'].num_columns\n",
    "        model_hparams = {'c_in': c_in} \n",
    "\n",
    "        fold_outputs = do_fold(train_ds, test_ds, \n",
    "            model_name, \n",
    "            model_hparams,\n",
    "            deterministic=deterministic, \n",
    "            log_prefix=log_prefix+f'_fold{f}')\n",
    "            \n",
    "        outputs[test_idx] = fold_outputs['proba'].cpu()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    labels = torch.Tensor(dataset.get_all_labels())\n",
    "    run_metrics = get_metrics(outputs, labels, metrics_name)\n",
    "\n",
    "    return outputs, run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 761.39it/s]\n"
     ]
    }
   ],
   "source": [
    "pose_extractor = ConflabPoseExtractor(conflab_pose_path)\n",
    "pose_extractor.load_from_pickle('../tracks.pkl')\n",
    "# make windowed examples using the pose tracks.\n",
    "examples = pose_extractor.make_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('paper_runs')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    f_handler = logging.FileHandler('paper_runs.csv', mode='w')\n",
    "    logger.addHandler(f_handler)\n",
    "\n",
    "inputs_map = {\n",
    "    'all': None,\n",
    "    'accel': ['accelX', 'accelY', 'accelZ'],\n",
    "    'accel-gyro': ['accelX', 'accelY', 'accelZ', 'gyrX', 'gyrY', 'gyrZ'],\n",
    "    'gyro':  ['gyrX', 'gyrY', 'gyrZ'],\n",
    "    'mag':  ['magX', 'magY', 'magZ'],\n",
    "    'rot':  ['rotA', 'rotB', 'rotC', 'rotD']\n",
    "}\n",
    "\n",
    "def do_paper_runs():\n",
    "    results = {}\n",
    "    # for model_name in ['minirocket', 'inception', 'resnet']:\n",
    "    for model_name in ['resnet']:\n",
    "        model_results = {}\n",
    "\n",
    "        for features in ['accel']:\n",
    "\n",
    "            label_extractor = ConflabLabelExtractor(os.path.join(conflab_speaking_status_path, 'speaking'))\n",
    "            sensor_extractor = ConflabAccelExtractor(midge_data_path, \n",
    "                columns=inputs_map[features])\n",
    "\n",
    "            dataset = ConflabDataset(examples, {\n",
    "                'accel': sensor_extractor,\n",
    "                'label': label_extractor\n",
    "            })\n",
    "\n",
    "            seed=22\n",
    "            pl.utilities.seed.seed_everything(seed, workers=True)\n",
    "            proba, metrics = do_run(\n",
    "                dataset, \n",
    "                model_name, \n",
    "                random_state=seed, \n",
    "                metrics_name='binary', \n",
    "                deterministic=True, \n",
    "                log_prefix=f\"{model_name}_{features}\")\n",
    "            model_results[features] = metrics\n",
    "            pd.DataFrame(proba.numpy()).to_csv(\n",
    "                os.path.join('outputs', f\"{model_name}_{features}.csv\"),\n",
    "                header=False,\n",
    "                index=False)\n",
    "            logger.info(f\"{model_name}, {features}, {metrics['auc']}, {metrics['acc']}\")\n",
    "        results[model_name] = model_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resnet': {'accel': {'auc': 0.800803211881501,\n",
       "   'acc': 0.766731962128632,\n",
       "   'correct': 32879}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_paper_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'minirocket': {'gyro': {'auc': 0.7646956950173082,\n",
    "   'acc': 0.7155449839093326,\n",
    "   'correct': 30684},\n",
    "  'mag': {'auc': 0.6102750641144488,\n",
    "   'acc': 0.6557996362109976,\n",
    "   'correct': 28122},\n",
    "  'rot': {'auc': 0.7259683260563268,\n",
    "   'acc': 0.696119583974628,\n",
    "   'correct': 29851}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{asdf}\n",
      "\\label{asdf}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &   AUC & Accuracy \\\\\n",
      "\\midrule\n",
      "gyro & 0.765 &    0.716 \\\\\n",
      "mag  & 0.610 &    0.656 \\\\\n",
      "rot  & 0.726 &    0.696 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3087/3476099220.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(pd.DataFrame(res['minirocket']).transpose()[['auc', 'acc']].to_latex(\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(res['minirocket']).transpose()[['auc', 'acc']].to_latex(\n",
    "    header=['AUC', 'Accuracy'],\n",
    "    float_format=\"%.3f\",\n",
    "    caption='asdf',\n",
    "    label='asdf'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ff63645dd16f55240e07095d3c46f4fac3f89ef16802cfaceca713f6cf38dfb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
